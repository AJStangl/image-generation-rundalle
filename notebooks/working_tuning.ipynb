{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/398 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c7382d679084c8188124bba8edc1811"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "◼️ Malevich is 1.3 billion params model from the family GPT3-like, that uses Russian language and text+image multi-modality.\n",
      "Working with z of shape (1, 256, 32, 32) = 262144 dimensions.\n",
      "vae --> ready\n",
      "tokenizer --> ready\n",
      "['/data/workspace/images\\\\20210609_131726.jpg', '/data/workspace/images\\\\20210609_205421.jpg', '/data/workspace/images\\\\20210610_141049.jpg', '/data/workspace/images\\\\20210722_220808.jpg', '/data/workspace/images\\\\20210723_093702.jpg', '/data/workspace/images\\\\20210723_094405.jpg', '/data/workspace/images\\\\20210723_132804.jpg', '/data/workspace/images\\\\20210723_140337.jpg', '/data/workspace/images\\\\20210723_140345.jpg', '/data/workspace/images\\\\20210723_140542.jpg', '/data/workspace/images\\\\20210723_154807.jpg', '/data/workspace/images\\\\20210723_154810.jpg', '/data/workspace/images\\\\20210723_154927(1).jpg', '/data/workspace/images\\\\20210723_154927.jpg', '/data/workspace/images\\\\20210723_154932.jpg', '/data/workspace/images\\\\20210723_154946.jpg', '/data/workspace/images\\\\20210723_160020.jpg', '/data/workspace/images\\\\20210723_160022.jpg', '/data/workspace/images\\\\20210723_160025.jpg', '/data/workspace/images\\\\20210723_160026.jpg', '/data/workspace/images\\\\20210723_160043.jpg', '/data/workspace/images\\\\20210723_160048.jpg', '/data/workspace/images\\\\20210723_160158.jpg', '/data/workspace/images\\\\20210723_160200.jpg', '/data/workspace/images\\\\20210723_160206.jpg']\n",
      "A happy couple /data/workspace/images\\20210609_131726.jpg\n",
      "A happy couple /data/workspace/images\\20210609_205421.jpg\n",
      "A happy couple /data/workspace/images\\20210610_141049.jpg\n",
      "A happy couple /data/workspace/images\\20210722_220808.jpg\n",
      "A happy couple /data/workspace/images\\20210723_093702.jpg\n",
      "A happy couple /data/workspace/images\\20210723_094405.jpg\n",
      "A happy couple /data/workspace/images\\20210723_132804.jpg\n",
      "A happy couple /data/workspace/images\\20210723_140337.jpg\n",
      "A happy couple /data/workspace/images\\20210723_140345.jpg\n",
      "A happy couple /data/workspace/images\\20210723_140542.jpg\n",
      "A happy couple /data/workspace/images\\20210723_154807.jpg\n",
      "A happy couple /data/workspace/images\\20210723_154810.jpg\n",
      "A happy couple /data/workspace/images\\20210723_154927(1).jpg\n",
      "A happy couple /data/workspace/images\\20210723_154927.jpg\n",
      "A happy couple /data/workspace/images\\20210723_154932.jpg\n",
      "A happy couple /data/workspace/images\\20210723_154946.jpg\n",
      "A happy couple /data/workspace/images\\20210723_160020.jpg\n",
      "A happy couple /data/workspace/images\\20210723_160022.jpg\n",
      "A happy couple /data/workspace/images\\20210723_160025.jpg\n",
      "A happy couple /data/workspace/images\\20210723_160026.jpg\n",
      "A happy couple /data/workspace/images\\20210723_160043.jpg\n",
      "A happy couple /data/workspace/images\\20210723_160048.jpg\n",
      "A happy couple /data/workspace/images\\20210723_160158.jpg\n",
      "A happy couple /data/workspace/images\\20210723_160200.jpg\n",
      "A happy couple /data/workspace/images\\20210723_160206.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "finetuning goes brrr:   0%|          | 0/250 [00:00<?, ?it/s]\u001B[AC:\\ProgramData\\Anaconda3\\envs\\Imaging\\lib\\site-packages\\rudalle\\dalle\\model.py:94: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  row_ids = torch.arange(past_length, input_shape[-1] + past_length,\n",
      "\n",
      "finetuning goes brrr:   0%|          | 1/250 [00:16<1:09:06, 16.65s/it]\u001B[A\n",
      "finetuning goes brrr:   0%|          | 1/250 [00:16<1:09:06, 16.65s/it, loss=6.27]\u001B[AC:\\ProgramData\\Anaconda3\\envs\\Imaging\\lib\\site-packages\\PIL\\Image.py:3035: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "finetuning goes brrr:   0%|          | 1/250 [00:22<1:31:56, 22.16s/it, loss=6.27]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed with CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 11.18 GiB total capacity; 10.20 GiB already allocated; 162.12 MiB free; 10.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ruclip\n",
    "from rudalle import get_rudalle_model, get_vae, get_tokenizer, get_realesrgan\n",
    "from rudalle.pipelines import generate_images, show, cherry_pick_by_ruclip, super_resolution\n",
    "from rudalle.utils import seed_everything\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "from translatepy import Translate\n",
    "import wandb\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class Args:\n",
    "\tdef __init__(self, model, epoch_amt, learning_rate):\n",
    "\t\tself.text_seq_length = model.get_param('text_seq_length')\n",
    "\t\tself.total_seq_length = model.get_param('total_seq_length')\n",
    "\t\tself.epochs = 10\n",
    "\t\tself.save_dir = '/data/workspace/checkpoints'\n",
    "\t\tself.model_name = 'local_model'\n",
    "\t\tself.save_every = 1000\n",
    "\t\tself.prefix_length = 5\n",
    "\t\tself.bs = 1\n",
    "\t\tself.clip = 0.24\n",
    "\t\tself.lr = learning_rate\n",
    "\t\tself.warmup_steps = 10\n",
    "\t\tself.wandb = False\n",
    "\n",
    "ts = Translate()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = get_rudalle_model('Malevich', pretrained=True, fp16=True, device=device)\n",
    "\n",
    "torch_args = Args(model=model, epoch_amt=10, learning_rate=1e-4)\n",
    "\n",
    "model_path = os.path.join(torch_args.save_dir, f\"{torch_args.model_name}_dalle_last.pt\")\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "\tmodel.load_state_dict(torch.load(model_path))\n",
    "\n",
    "vae = get_vae().to('cuda')\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "class RuDalleDataset(Dataset):\n",
    "\tclip_filter_thr = 0.24\n",
    "\n",
    "\tdef __init__(\n",
    "\t\t\tself,\n",
    "\t\t\tcsv_path,\n",
    "\t\t\ttokenizer,\n",
    "\t\t\tresize_ratio=0.75,\n",
    "\t\t\tshuffle=True,\n",
    "\t\t\tload_first=None,\n",
    "\t\t\tcaption_score_thr=0.6\n",
    "\t):\n",
    "\t\t\"\"\" tokenizer - object with methods tokenizer_wrapper.BaseTokenizerWrapper \"\"\"\n",
    "\n",
    "\t\tself.text_seq_length = model.get_param('text_seq_length')\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.target_image_size = 256\n",
    "\t\tself.image_size = 256\n",
    "\t\tself.samples = []\n",
    "\n",
    "\t\tself.image_transform = T.Compose([\n",
    "\t\t\tT.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "\t\t\tT.RandomResizedCrop(\n",
    "\t\t\t\tself.image_size,\n",
    "\t\t\t\tscale=(1., 1.),  # в train было scale=(0.75., 1.),\n",
    "\t\t\t\tratio=(1., 1.)\n",
    "\t\t\t),\n",
    "\t\t\tT.ToTensor()\n",
    "\t\t])\n",
    "\n",
    "\t\tdf = pd.read_csv(csv_path)\n",
    "\t\tfor caption, image_path in zip(df['caption'], df['name']):\n",
    "\t\t\tprint(caption, image_path)\n",
    "\t\t\tself.samples.append([image_path, caption])\n",
    "\t\tif shuffle:\n",
    "\t\t\tnp.random.shuffle(self.samples)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.samples)\n",
    "\n",
    "\tdef load_image(self, image_path):\n",
    "\t\timage = PIL.Image.open(image_path)\n",
    "\t\treturn image\n",
    "\n",
    "\tdef __getitem__(self, item):\n",
    "\t\titem = item % len(self.samples)  # infinite loop, modulo dataset size\n",
    "\t\timage_path, text = self.samples[item]\n",
    "\t\ttry:\n",
    "\t\t\timage = self.load_image(image_path)\n",
    "\t\t\timage = self.image_transform(image).to(device)\n",
    "\t\texcept Exception as err:  # noqa\n",
    "\t\t\tprint(err)\n",
    "\t\t\trandom_item = random.randint(0, len(self.samples) - 1)\n",
    "\t\t\treturn self.__getitem__(random_item)\n",
    "\t\ttext = tokenizer.encode_text(text, text_seq_length=self.text_seq_length).squeeze(0).to(device)\n",
    "\t\treturn text, image\n",
    "\n",
    "file_selector_glob = \"/data/workspace/images/*\"\n",
    "\n",
    "data_path = '/data/workspace/data_desc.csv'\n",
    "\n",
    "input_files = glob.glob(file_selector_glob, recursive=True)[0:5]\n",
    "print(input_files)\n",
    "\n",
    "with open(\"/data/workspace/data_desc.csv\", 'w', encoding='utf-8') as f:\n",
    "\theader = \"caption,name\\n\"\n",
    "\tf.write(header)\n",
    "\tfor elem in input_files:\n",
    "\t\tgeneric = \"A happy couple\"\n",
    "\t\ttranslated = ts.translate(generic, \"ru\").result\n",
    "\t\t# foo = str(translated)\n",
    "\t\tf.write(f\"{generic},{elem}\\n\")\n",
    "\n",
    "st = RuDalleDataset(tokenizer=tokenizer, csv_path=data_path)\n",
    "\n",
    "train_dataloader = DataLoader(st, batch_size=torch_args.bs, shuffle=True, drop_last=True)\n",
    "torch_args.wandb = False\n",
    "optimizer = AdamW(model.parameters(), lr=torch_args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "\toptimizer, max_lr=torch_args.lr,\n",
    "\tfinal_div_factor=500,\n",
    "\tsteps_per_epoch=len(train_dataloader), epochs=torch_args.epochs\n",
    ")\n",
    "\n",
    "def freeze(\n",
    "    model,\n",
    "    freeze_emb=True,\n",
    "    freeze_ln=False,\n",
    "    freeze_attn=False,\n",
    "    freeze_ff=True,\n",
    "    freeze_other=True,\n",
    "):\n",
    "    for name, p in model.module.named_parameters():\n",
    "        name = name.lower()\n",
    "        if 'ln' in name or 'norm' in name:\n",
    "            p.requires_grad = not freeze_ln\n",
    "        elif 'embeddings' in name:\n",
    "            p.requires_grad = not freeze_emb\n",
    "        elif 'mlp' in name:\n",
    "            p.requires_grad = not freeze_ff\n",
    "        elif 'attn' in name:\n",
    "            p.requires_grad = not freeze_attn\n",
    "        else:\n",
    "            p.requires_grad = not freeze_other\n",
    "    return model\n",
    "\n",
    "def train(model, args: Args, train_dataloader: RuDalleDataset):\n",
    "\t\"\"\"\n",
    "    args - arguments for training\n",
    "\n",
    "    train_dataloader - RuDalleDataset class with text - image pair in batch\n",
    "    \"\"\"\n",
    "\tloss_logs = []\n",
    "\ttry:\n",
    "\t\tprogress = tqdm(total=(args.epochs * len(input_files)), desc='finetuning goes brrr')\n",
    "\t\tsave_counter = 0\n",
    "\t\tfor epoch in range(args.epochs):\n",
    "\t\t\tfor text, images in train_dataloader:\n",
    "\t\t\t\tdevice = model.get_param('device')\n",
    "\t\t\t\tsave_counter += 1\n",
    "\t\t\t\tmodel.zero_grad()\n",
    "\t\t\t\tattention_mask = torch.tril(\n",
    "\t\t\t\t\ttorch.ones(\n",
    "\t\t\t\t\t\t(args.bs, 1, args.total_seq_length, args.total_seq_length),\n",
    "\t\t\t\t\t\tdevice=device\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t)\n",
    "\t\t\t\timage_input_ids = vae.get_codebook_indices(images)\n",
    "\n",
    "\t\t\t\tinput_ids = torch.cat((text, image_input_ids), dim=1)\n",
    "\t\t\t\t_, loss = forward(\n",
    "\t\t\t\t\tmodel.module, input_ids, attention_mask.half(),\n",
    "\t\t\t\t\treturn_loss=True, use_cache=False, gradient_checkpointing=6\n",
    "\t\t\t\t)\n",
    "\t\t\t\tloss = loss[\"image\"]\n",
    "\t\t\t\t# train step\n",
    "\t\t\t\tloss.backward()\n",
    "\n",
    "\t\t\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\t\tscheduler.step()\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\t# save every here\n",
    "\t\t\t\tif save_counter % args.save_every == 0:\n",
    "\t\t\t\t\tprint(f'Saving checkpoint here {args.model_name}_dalle_{save_counter}.pt')\n",
    "\n",
    "\t\t\t\t\tplt.plot(loss_logs)\n",
    "\t\t\t\t\tplt.show()\n",
    "\t\t\t\t\ttorch.save(\n",
    "\t\t\t\t\t\tmodel.state_dict(),\n",
    "\t\t\t\t\t\tos.path.join(args.save_dir, f\"{args.model_name}_dalle_{save_counter}.pt\")\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\tif args.wandb:\n",
    "\t\t\t\t\targs.wandb.log({\"loss\": loss.item()})\n",
    "\t\t\t\tloss_logs += [loss.item()]\n",
    "\t\t\t\tprogress.update()\n",
    "\t\t\t\tprogress.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "\t\tprint(f'Completly tuned and saved here  {args.model_name}_dalle_last.pt')\n",
    "\n",
    "\t\tplt.plot(loss_logs)\n",
    "\t\tplt.show()\n",
    "\n",
    "\t\ttorch.save(\n",
    "\t\t\tmodel.state_dict(),\n",
    "\t\t\tos.path.join(args.save_dir, f\"{args.model_name}_dalle_last.pt\")\n",
    "\t\t)\n",
    "\n",
    "\texcept KeyboardInterrupt:\n",
    "\t\tprint(\n",
    "\t\t\tf'What for did you stopped? Please change model_path to /{args.save_dir}/{args.model_name}_dalle_Failed_train.pt')\n",
    "\t\tplt.plot(loss_logs)\n",
    "\t\tplt.show()\n",
    "\n",
    "\t\ttorch.save(\n",
    "\t\t\tmodel.state_dict(),\n",
    "\t\t\tos.path.join(args.save_dir, f\"{args.model_name}_dalle_Failed_train.pt\")\n",
    "\t\t)\n",
    "\texcept Exception as err:\n",
    "\t\tprint(f'Failed with {err}')\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from rudalle.dalle.utils import exists, is_empty\n",
    "\n",
    "\n",
    "# idk why but this is necessary\n",
    "class Layer(torch.nn.Module):\n",
    "    def __init__(self, x, f, *args, **kwargs):\n",
    "        super(Layer, self).__init__()\n",
    "        self.x = x\n",
    "        self.f = f\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.f(self.x(x, *self.args, **self.kwargs))\n",
    "\n",
    "\n",
    "def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        return_loss=False,\n",
    "        use_cache=False,\n",
    "        gradient_checkpointing=False\n",
    "):\n",
    "    text = input_ids[:, :self.text_seq_length]\n",
    "    text_range = torch.arange(self.text_seq_length)\n",
    "    text_range += (self.vocab_size - self.text_seq_length)\n",
    "    text_range = text_range.to(self.device)\n",
    "    text = torch.where(text == 0, text_range, text)\n",
    "    # some hardcode :)\n",
    "    text = F.pad(text, (1, 0), value=2)\n",
    "    text_embeddings = self.text_embeddings(text) + \\\n",
    "        self.text_pos_embeddings(torch.arange(text.shape[1], device=self.device))\n",
    "\n",
    "    image_input_ids = input_ids[:, self.text_seq_length:]\n",
    "\n",
    "    if exists(image_input_ids) and not is_empty(image_input_ids):\n",
    "        image_embeddings = self.image_embeddings(image_input_ids) + \\\n",
    "            self.get_image_pos_embeddings(image_input_ids, past_length=0)\n",
    "        embeddings = torch.cat((text_embeddings, image_embeddings), dim=1)\n",
    "    else:\n",
    "        embeddings = text_embeddings\n",
    "    # some hardcode :)\n",
    "    if embeddings.shape[1] > self.total_seq_length:\n",
    "        embeddings = embeddings[:, :-1]\n",
    "\n",
    "    alpha = 0.1\n",
    "    embeddings = embeddings * alpha + embeddings.detach() * (1 - alpha)\n",
    "\n",
    "    attention_mask = attention_mask[:, :, :embeddings.shape[1], :embeddings.shape[1]]\n",
    "    t = self.transformer\n",
    "    layers = []\n",
    "    layernorms = []\n",
    "    if not layernorms:\n",
    "        norm_every = 0\n",
    "    else:\n",
    "        norm_every = len(t.layers) // len(layernorms)\n",
    "    for i in range(len(t.layers)):\n",
    "        layers.append(Layer(\n",
    "            t.layers[i],\n",
    "            lambda x:\n",
    "                x[0] * layernorms[i // norm_every][0] +\n",
    "                layernorms[i // norm_every][1] if norm_every and i % norm_every == 0 else x[0],\n",
    "            torch.mul(attention_mask, t._get_layer_mask(i)[:attention_mask.size(2), :attention_mask.size(3), ]),\n",
    "            use_cache=False\n",
    "        ))\n",
    "    if gradient_checkpointing:  # don't use this under any circumstances\n",
    "        # actually please do\n",
    "        # i just spent 3 hours debugging this\n",
    "        embeddings = torch.utils.checkpoint.checkpoint_sequential(layers, 6, embeddings)\n",
    "        transformer_output = embeddings\n",
    "        present_has_cache = False\n",
    "    else:\n",
    "        hidden_states = embeddings\n",
    "        for i in range(len(t.layers)):\n",
    "            mask = torch.mul(attention_mask, t._get_layer_mask(i)[:attention_mask.size(2), :attention_mask.size(3)])\n",
    "            hidden_states, present_has_cache = t.layers[i](hidden_states, mask, use_cache=use_cache)\n",
    "        transformer_output = hidden_states\n",
    "    transformer_output = self.transformer.final_layernorm(transformer_output)\n",
    "\n",
    "    logits = self.to_logits(transformer_output)\n",
    "    if return_loss is False:\n",
    "        return logits, present_has_cache\n",
    "\n",
    "    labels = torch.cat((text[:, 1:], image_input_ids), dim=1).contiguous().long()\n",
    "    logits = rearrange(logits, 'b n c -> b c n')\n",
    "\n",
    "    text_logits = logits[:, :self.vocab_size, :self.text_seq_length].contiguous().float()\n",
    "    image_logits = logits[:, self.vocab_size:, self.text_seq_length:].contiguous().float()\n",
    "\n",
    "    loss_text = F.cross_entropy(\n",
    "        text_logits,\n",
    "        labels[:, :self.text_seq_length])\n",
    "    loss_img = F.cross_entropy(\n",
    "        image_logits,\n",
    "        labels[:, self.text_seq_length:])\n",
    "\n",
    "    loss = (loss_text + self.loss_img_weight * loss_img) / (self.loss_img_weight + 1)\n",
    "    return loss, {'text': loss_text.data.detach().float(), 'image': loss_img}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run training on model\n",
    "train(model, torch_args, train_dataloader)\n",
    "\n",
    "#freeze params to\n",
    "model = freeze(\n",
    "    model=model,\n",
    "    freeze_emb=False,\n",
    "    freeze_ln=False,\n",
    "    freeze_attn=True,\n",
    "    freeze_ff=True,\n",
    "    freeze_other=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}