{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ruclip\n",
    "from rudalle import get_rudalle_model, get_vae, get_tokenizer, get_realesrgan\n",
    "from rudalle.pipelines import generate_images, show, cherry_pick_by_ruclip, super_resolution\n",
    "from rudalle.utils import seed_everything\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "from translatepy import Translate\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PathConfiguration(object):\n",
    "    def __init__(self):\n",
    "        self.rudalle_cache_dir = '/data/workspace/rudalle'\n",
    "        self.checkpoint_dir = '/data/workspace/checkpoints'\n",
    "        self.training_image_path = '/data/workspace/images'\n",
    "        self.data_path = '/data/workspace/data_desc.csv'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TrainingConfiguration(object):\n",
    "    def __init__(self, model_instance):\n",
    "        self.model = model_instance\n",
    "        self.model_name = 'tuned_model'\n",
    "        self.text_seq_length = self.model.get_param('text_seq_length')\n",
    "        self.total_seq_length = self.model.get_param('total_seq_length')\n",
    "        self.save_every = 200\n",
    "        self.prefix_length = 10\n",
    "        self.bs = 1\n",
    "        self.clip = 0.24\n",
    "        self.lr = 1e-4\n",
    "        self.warmup_steps = 50\n",
    "        self.epochs = 10\n",
    "        self.wandb = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RuDalleDataset(Dataset):\n",
    "    clip_filter_thr = 0.24\n",
    "    def __init__(\n",
    "            self,\n",
    "            file_path,\n",
    "            csv_path,\n",
    "            tokenizer,\n",
    "            resize_ratio=0.75,\n",
    "            shuffle=True,\n",
    "            load_first=None,\n",
    "            caption_score_thr=0.6\n",
    "    ):\n",
    "\n",
    "        self.text_seq_length = model.get_param('text_seq_length')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.target_image_size = 256\n",
    "        self.image_size=256\n",
    "        self.samples = []\n",
    "\n",
    "\n",
    "        self.image_transform = T.Compose([\n",
    "                T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "                T.RandomResizedCrop(self.image_size,\n",
    "                                    scale=(1., 1.), # в train было scale=(0.75., 1.),\n",
    "                                    ratio=(1., 1.)),\n",
    "                T.ToTensor()\n",
    "            ])\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "        for caption, f_path in zip(df['caption'], df['name']):\n",
    "            if os.path.isfile(f'{file_path}/{f_path}'):\n",
    "                #Note: You may want to perform a translation here on the caption... I don't see a difference\n",
    "              self.samples.append([file_path, f_path, caption])\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.samples)\n",
    "            print('Shuffled')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def load_image(self, file_path, img_name):\n",
    "        image = PIL.Image.open(f'{file_path}/{img_name}')\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        item = item % len(self.samples)  # infinite loop, modulo dataset size\n",
    "        file_path, img_name, text = self.samples[item]\n",
    "        try:\n",
    "          image = self.load_image(file_path, img_name)\n",
    "          image = self.image_transform(image).to(device)\n",
    "        except Exception as err:  # noqa\n",
    "            print(err)\n",
    "            random_item = random.randint(0, len(self.samples) - 1)\n",
    "            return self.__getitem__(random_item)\n",
    "        text =  tokenizer.encode_text(text, text_seq_length=self.text_seq_length).squeeze(0).to(device)\n",
    "        return text, image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path_configuration: PathConfiguration = PathConfiguration()\n",
    "\n",
    "translation_engine = Translate()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = get_rudalle_model('Malevich', pretrained=True, fp16=True, device=device, cache_dir=path_configuration.rudalle_cache_dir)\n",
    "\n",
    "training_configuration: TrainingConfiguration = TrainingConfiguration(model)\n",
    "\n",
    "vae = get_vae(dwt=True).to(device)\n",
    "\n",
    "model_path = os.path.join(path_configuration.checkpoint_dir, f\"{training_configuration.model_name}_dalle_last.pt\")\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "tokenizer = get_tokenizer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_files = [os.path.join(path_configuration.training_image_path, item) for item in os.listdir(path_configuration.training_image_path)]\n",
    "\n",
    "with open(path_configuration.data_path, 'w',encoding=\"utf-8\") as f:\n",
    "    header = \"caption,name\\n\"\n",
    "    f.write(header)\n",
    "    for elem in input_files:\n",
    "        foo = os.path.split(elem)[-1]\n",
    "        generic = 'A red head woman'\n",
    "        translated = translation_engine.translate(generic, source_language='EN', destination_language=\"ru\").result\n",
    "        f.write(f\"{generic},{foo}\\n\")\n",
    "        f.write(f\"{translated},{foo}\\n\")\n",
    "\n",
    "# Training\n",
    "st = RuDalleDataset(file_path=path_configuration.training_image_path, csv_path=path_configuration.data_path, tokenizer=tokenizer)\n",
    "\n",
    "training_configuration.wandb = False\n",
    "\n",
    "train_dataloader = DataLoader(st, batch_size=training_configuration.bs, shuffle=True, drop_last=True)\n",
    "\n",
    "model.train()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = training_configuration.lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=training_configuration.lr,\n",
    "    final_div_factor=500,\n",
    "    steps_per_epoch=len(train_dataloader),\n",
    "    epochs=training_configuration.epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def freeze(\n",
    "    model,\n",
    "    freeze_emb=True,\n",
    "    freeze_ln=False,\n",
    "    freeze_attn=False,\n",
    "    freeze_ff=True,\n",
    "    freeze_other=True,\n",
    "):\n",
    "    for name, p in model.module.named_parameters():\n",
    "        name = name.lower()\n",
    "        if 'ln' in name or 'norm' in name:\n",
    "            p.requires_grad = not freeze_ln\n",
    "        elif 'embeddings' in name:\n",
    "            p.requires_grad = not freeze_emb\n",
    "        elif 'mlp' in name:\n",
    "            p.requires_grad = not freeze_ff\n",
    "        elif 'attn' in name:\n",
    "            p.requires_grad = not freeze_attn\n",
    "        else:\n",
    "            p.requires_grad = not freeze_other\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(model, training_args: TrainingConfiguration, path_args: PathConfiguration, train_dataloader: RuDalleDataset):\n",
    "  loss_logs = []\n",
    "  try:\n",
    "    progress = tqdm(total=training_args.epochs * len(train_dataloader), desc='finetuning goes brrr')\n",
    "    save_counter = 0\n",
    "    for epoch in range(training_args.epochs):\n",
    "\n",
    "      for text, images in train_dataloader:\n",
    "        device = model.get_param('device')\n",
    "        save_counter+=1\n",
    "        model.zero_grad()\n",
    "        attention_mask = torch.tril(torch.ones((training_args.bs, 1, training_args.total_seq_length, training_args.total_seq_length), device=device))\n",
    "        image_input_ids = vae.get_codebook_indices(images)\n",
    "\n",
    "        input_ids = torch.cat((text, image_input_ids), dim=1)\n",
    "        loss, loss_values = model.forward(input_ids, attention_mask, return_loss=True)\n",
    "        #train step\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),training_args.clip)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        #save every here\n",
    "        if save_counter % training_args.save_every == 0:\n",
    "          print(f'Saveing checkpoint here {training_args.model_name}_dalle_{save_counter}.pt')\n",
    "\n",
    "          plt.plot(loss_logs)\n",
    "          plt.show()\n",
    "          torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(path_args.checkpoint_dir, f\"{training_args.model_name}_dalle_{save_counter}.pt\")\n",
    "                    )\n",
    "        if training_configuration.wandb:\n",
    "          wandb.log({\"loss\":  loss.item()})\n",
    "        loss_logs+=[loss.item()]\n",
    "        progress.update()\n",
    "        progress.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    print(f'Completed tuning and saved to: {training_configuration.model_name}_dalle_last.pt')\n",
    "\n",
    "    plt.plot(loss_logs)\n",
    "    plt.show()\n",
    "\n",
    "    torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(path_configuration.checkpoint_dir, f\"{path_configuration.checkpoint_dir}/{training_configuration.model_name}_dalle_last.pt\"))\n",
    "\n",
    "  except KeyboardInterrupt:\n",
    "    print(f'What for did you stopped? Please change model_path to /{path_configuration.checkpoint_dir}/{training_args.model_name}_dalle_Failed_train.pt')\n",
    "    plt.plot(loss_logs)\n",
    "    plt.show()\n",
    "\n",
    "    torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(path_args.checkpoint_dir,f\"{path_args.checkpoint_dir}/{training_args.model_name}_dalle_Failed_train.pt\"))\n",
    "  except Exception as err:\n",
    "    print(f'Failed with {err}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(model, training_configuration, path_configuration, train_dataloader)\n",
    "\n",
    "model = freeze(model = model,\n",
    "    freeze_emb=False,\n",
    "    freeze_ln=False,\n",
    "    freeze_attn=True,\n",
    "    freeze_ff=True,\n",
    "    freeze_other=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}